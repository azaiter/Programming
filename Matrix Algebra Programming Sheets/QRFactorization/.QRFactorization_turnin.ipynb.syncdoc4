{"notebook_name":"QRFactorization_turnin.ipynb"}
"# Finding eigenvalues using repeated QR Factorization"︡{"metadata":{"collapsed":false},"cell_type":"markdown"}
"To review QR factorization:  in class we learned that given a __nonsingular__ square matrix $A$, we can write the $A$ as a product of two matrices: $A = QR$, where $Q$ is __orthogonal__ and $R$ is __upper triangular__.  In Julia, we can find an approximate numerical QR factorization of a given matrix $A$ easily using the `qr` function:"︡{"metadata":{},"cell_type":"markdown"}
"A = [1 2; 3 4]"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Int64,2}:\n 1  2\n 3  4"},"metadata":{}}]}
"Q, R = qr(A)"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(\n2x2 Array{Float64,2}:\n -0.316228  -0.948683\n -0.948683   0.316228,\n\n2x2 Array{Float64,2}:\n -3.16228  -4.42719 \n  0.0      -0.632456)"},"metadata":{}}]}
"We can verify that $Q$ is an orthogonal matrix:"︡{"metadata":{},"cell_type":"markdown"}
"Q*Q'"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 1.0  0.0\n 0.0  1.0"},"metadata":{}}]}
"Q'*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 1.0  0.0\n 0.0  1.0"},"metadata":{}}]}
"We can also easily verify that $QR = A$:"︡{"metadata":{},"cell_type":"markdown"}
"Q*R"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 1.0  2.0\n 3.0  4.0"},"metadata":{}}]}
"What is even more interesting is the fact that if we change the order in which we multiply $Q$ and $R$, we get a matrix that is __similar to__ $A$, with $Q$ being the similarity transformation:\n\n$$ Q^{-1} A Q = Q^{-1} (Q R) Q = (Q^{-1} Q) (R Q) = I R Q = RQ $$\n\nThe matrix $Q$ is not only a similarity transformation, since it is an orthogonal matrix, it is an _orthogonal similarity transformation_, so $RQ = Q^t A Q$:"︡{"metadata":{},"cell_type":"markdown"}
"R*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.2   1.6\n 0.6  -0.2"},"metadata":{}}]}
"Q'*A*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.2   1.6\n 0.6  -0.2"},"metadata":{}}]}
"In class we saw that similar matrices have the same characteristic polynomial, and therefore the same eigenvalues, so $RQ$ has the same eigenvalues as $A$.\n\nWe can repeat the process:  find the QR factorization of the matrix $RQ$, and maltiply the new $Q$ and the new $R$ in the reverse order, to get new $RQ$.  This new $RQ$ is similar to the old $RQ$ and therefore to $A$, so it has the same eigenvalues as $A$.  We can keep doing it over and over:"︡{"metadata":{},"cell_type":"markdown"}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37956    -0.956204\n 0.0437956  -0.379562"},"metadata":{}}]}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37175      1.00303 \n 0.00302648  -0.371753"},"metadata":{}}]}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37232      -0.99979 \n 0.000209766  -0.372318"},"metadata":{}}]}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37228      1.00001 \n 1.45359e-5  -0.372279"},"metadata":{}}]}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37228     -0.999999\n 1.00729e-6  -0.372281"},"metadata":{}}]}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37228     1.0     \n 6.9802e-8  -0.372281"},"metadata":{}}]}
"Q, R = qr(R*Q)\nR*Q"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"2x2 Array{Float64,2}:\n 5.37228     -1.0     \n 4.83705e-9  -0.372281"},"metadata":{}}]}
"If you look at the last few values of $QR$, you may notice that there is a pattern.  The two diagonal entries do not seem to be changing much.  The entry in the lower left corner seem to be getting really small (in order $10^{-9}$ in the last matrix).  And the entry in the upper right corner seem to be oscillating between roughly $-1$ and $1$."︡{"metadata":{},"cell_type":"markdown"}
"This is actually not happening by a chance.  This is guaranteed to happen as long as the original matrix $A$ (and therefore all the subsequent matrices $RQ$) have $n$ distinct eigenvalues.  It may also happen if they do not have $n$ distinct eigenvalues, but it is not guaranteed.  What is happening here is the subsequent matrices $RQ$ converge to an upper triangular matrix, let's call it $T$.  Since they all have the same eigenvalues, the matrix $T$ will also have the same eigenvalues."︡{"metadata":{},"cell_type":"markdown"}
"In your written homework that accompanies this homework, one of the exercises asks you to prove that eigenvalues of an upper triangular matrix are the entries on the diagonal.  That is not hard, and is based on the fact that if $T$ is upper triangular, $T - I\\lambda$ is also an upper triangular, and a determinant of an upper triangular matrix is very easy to calculate. "︡{"metadata":{},"cell_type":"markdown"}
"What it means for us is that as we calculate newer and newer versions of the $RQ$ matrix, __the entries on the diagonal are getting closer and closer to the eigenvalues of the matrix $A$__!  This means that if with enough repetition of the process, we can get pretty decent approximation of all the eigenvalues of $A$ without having to calculate any determinants or solving any polynomial equations! "︡{"metadata":{},"cell_type":"markdown"}
"In our case, it means that the eigenvalues of the matrix $$A = \\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}$$ are approximately:"︡{"metadata":{},"cell_type":"markdown"}
"diag(Q*R)"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"2-element Array{Float64,1}:\n  5.37228 \n -0.372281"},"metadata":{}}]}
"We can verify this by hand: the characteristic polynomial of $A$ is $(1-\\lambda)(4-\\lambda) - 6 = 4 - 5\\lambda - \\lambda^2 - 6 = \\lambda^2 - 5\\lambda - 2$. Solving the characteristic equation:\n\n$$\\begin{aligned}\n\\lambda^2 - 5\\lambda - 2 &= 0\\\\\n\\lambda^2 - 5\\lambda \\phantom{{} + \\frac{25}{4}} &= 2\\\\\n\\lambda^2 - 5\\lambda \\color{red}{ + \\frac{25}{4}} &= 2 \\color{red}{ + \\frac{25}{4}}\\\\\n\\left(\\lambda - \\frac{5}{2}\\right)^2 &= \\frac{33}{4}\\\\\n\\lambda &= \\frac{5}{2} \\pm \\frac{\\sqrt{33}}{2}\n\\end{aligned}$$\n"︡{"metadata":{},"cell_type":"markdown"}
"5/2 + sqrt(33)/2*[1,-1]"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"2-element Array{Float64,1}:\n  5.37228 \n -0.372281"},"metadata":{}}]}
"We can see that we got a very good approximation."︡{"metadata":{},"cell_type":"markdown"}
"Let's do the same thing for a larger matrix.  Take\n\n$$B = \\begin{bmatrix} 4 & 2 & -2 & 2\\\\ 1 & 3 & 1 & -1\\\\ 0 & 0 & 3 & 0\\\\ 1 & 1 & -3 & 5\\end{bmatrix}$$\n\nIt's eigenvalues are 2, 3, 4 and 6 (you can verify that, it is the same matrix as the one we used in class, except 2 in the third row is replaced by 4)"︡{"metadata":{},"cell_type":"markdown"}
"B = [4 2 -2 2; 1 3 1 -1; 0 0 3 0; 1 1 -3 5]"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"4x4 Array{Int64,2}:\n 4  2  -2   2\n 1  3   1  -1\n 0  0   3   0\n 1  1  -3   5"},"metadata":{}}]}
"for i = 1:20\n    Q, R = qr(B)\n    B = R*Q\n    println(\"Step $i: $(diag(B))\")\nend"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":"Step 1: [5.333333333333332,2.2222222222222223,4.9843077701847625,2.460136674259681]\nStep 2: [5.910447761194029,1.97996319771008,4.803959014286238,2.3056300268096512]\nStep 3: [6.090452261306533,1.9342010514823733,4.42802388494348,2.547322802267613]\nStep 4: [6.119205298013243,1.9399245524823037,4.229278085934171,2.711592063570279]\nStep 5: [6.101582585842426,1.9555509017416064,4.133289830678101,2.809576681737863]\nStep 6: [6.075976361820211,1.969444932980087,4.084919372658772,2.869659332540926]\nStep 7: [6.053781983188233,1.9795726751929508,4.058388908270744,2.908256433348065]\nStep 8: [6.037071067467034,1.9864729390562186,4.042339332232789,2.93411666124395]\nStep 9: [6.025198534132784,1.9910606747483457,4.031715675962619,2.952025115156243]\nStep 10: [6.016996432289655,1.994087943636204,4.02418326855292,2.9647323555212135]\nStep 11: [6.011412967619596,1.996083843162057,4.018596920377184,2.9739062688411564]\nStep 12: [6.007643261345437,1.997401786285195,4.014345426522212,2.9806095258471483]\nStep 13: [6.005110303275791,1.9982738844224393,4.011067580335975,2.985548231965783]\nStep 14: [6.00341325476006,1.9988520822467377,4.008527129064124,2.9892075339290693]\nStep 15: [6.0022782799373475,1.9992360262538509,4.006556349208554,2.9919293446002366]\nStep 16: [6.001520067496544,1.9994912802221723,4.005029488190631,2.9939591640906413]\nStep 17: [6.001013911505477,1.999661123753356,4.003849333599222,2.995475631141934]\nStep 18: [6.000676175861458,1.999774204400604,4.002939663686193,2.9966099560517336]\nStep 19: [6.000450887600436,1.999849524365239,4.002240445593641,2.997459142440674]\nStep 20: [6.0003006375940915,1.9998997074403886,4.001704426980075,2.9980952279854343]\n"}]}
"We can see that the the numbers are getting somewhat close to the eigenvalues.  Let's do 100 more iterations, this time printing only every 10th step:"︡{"metadata":{},"cell_type":"markdown"}
"for i = 1:100\n    Q, R = qr(B)\n    B = R*Q\n    if i % 10 == 0\n        println(\"Step $i: $(diag(B))\")\n    end\nend"︡{"metadata":{"collapsed":false,"trusted":false},"cell_type":"code","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"Step 10: [6.000005215067839,1.9999982497793214,4.000103688393436,2.99989284675939]\nStep 20: [6.000000090437714,1.9999878410846335,4.000018102438929,2.9999939660387103]\nStep 30: [6.000000001568326,1.9881302659902478,4.011870072234447,2.999999660206976]\nStep 40: [6.000000000027195,4.0304289805063425,1.9695710386055156,2.999999980860946]\nStep 50: [6.000000000000472,4.000034128922051,1.9999658858478717,2.9999999852296058]\nStep 60: [6.000000000000008,4.000000033775353,2.0000454964235534,2.999954469801086]\nStep 70: [6.000000000000001,4.0000000000578915,2.1314968426873016,2.8685031572548065]\nStep 80: [6.000000000000001,4.000000000001457,2.9980177011025417,2.0019822988960017]\nStep 90: [6.000000000000001,4.000000000000076,2.999999402682123,2.000000597317796]\nStep 100: [6.000000000000001,4.0,2.999999999820364,2.0000000001796296]\n"}]}
"It does not converge as fast as the first matrix, but you can see that after 120 iterations we have pretty good result."︡{"metadata":{},"cell_type":"markdown"}
"## Your homework"︡{"metadata":{},"cell_type":"markdown"}
"is simple: use this method to do exercises 37 a, b, d and e in section 5.1.  Do your work below in this notebook.  Use markdown cells to write which problem you are solving, and to write any comments or questions you may have."︡{"metadata":{},"cell_type":"markdown"}
""︡{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","execution_count":null,"outputs":[]}